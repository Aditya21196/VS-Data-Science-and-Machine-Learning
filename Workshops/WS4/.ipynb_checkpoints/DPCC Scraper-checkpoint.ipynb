{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have already discussed GET requests. This scraper uses POST requests as scraping the DPCC site data needs a lot of form submissions. I welcome contributions to improve the explanations in this notebook and even additional contributions that demonstrate webscraping newspapers data using GET requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates for searching\n",
    "adv1Template = \"http://www.dpccairdata.com/dpccairdata/display/AallAdvanceSearchCconc.php?stName=\"\n",
    "adv2Template = \"http://www.dpccairdata.com/dpccairdata/display/AallAdvanceSearchMet.php?stName=\"\n",
    "adv1List = ['NH3','BEN','CO','NO2','NO1','NOX','O3','pXy','SO2','Tol']\n",
    "adv2List = ['AT1','BP','PM10','PM25','RH','SR','WS','WD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More templates\n",
    "stNamesDict = {\n",
    "    \"Pooth Khurd, Bawana\" : \"UG9vdGhLaHVyZEJhd2FuYQ==\",\n",
    "    \"Nehru Nagar\" : \"TmVocnVOYWdhcg==\",\n",
    "    \"Jawaharlal Nehru Stadium\" : \"SkxOU3RhZGl1bQ==\",\n",
    "    \"Dr. Karni Singh Shooting Range\" : \"S2FybmlTaW5naFNob290aW5nUmFuZ2U=\",\n",
    "    \"Major Dhyan Chand National Stadium\" : \"TmF0aW9uYWxTdGFkaXVt\",\n",
    "    \"Patparganj\" : \"UGF0cGFyZ2Fuag==\",\n",
    "    \"Vivek Vihar\" : \"Vml2ZWtWaWhhcg==\",\n",
    "    \"Sonia Vihar\" : \"U29uaWFWaWhhcg==\",\n",
    "    \"Narela\" : \"TmFyZWxh\",\n",
    "    \"Najafgarh\" : \"TmFqYWZnYXJo\",\n",
    "    \"Rohini\" : \"Um9oaW5pU2VjdG9yMTY=\",\n",
    "    \"Okhla Phase-2\" : \"T2tobGFQaGFzZTI=\",\n",
    "    \"Ashok Vihar\" : \"QXNob2tWaWhhcg==\",\n",
    "    \"Wazirpur\" : \"V2F6aXJwdXI=\",\n",
    "    \"Jahangirpuri\" : \"SmFoYW5naXJwdXJp\",\n",
    "    \"Dwarka, Sector 8\" : \"RHdhcmthU2VjdHJvOA==\",\n",
    "    \"Pusa\" : \"UHVzYQ==\",\n",
    "    \"Sri Auribindo Marg\" : \"U3JpQXVyYmluZG9NYXJn\",\n",
    "    \"Mundka\" : \"TXVuZGth\",\n",
    "    \"Anand Vihar\" : \"QW5hbmRWaWhhcg==\",\n",
    "    \"Mandir Marg\" : \"TWFuZGlybWFyZw==\",\n",
    "    \"Punjabi Bagh\" : \"UHVuamFiaUJhZ2g=\",\n",
    "    \"R.K. Puram\" : \"UktQdXJhbQ==\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This piece of code below badly needs documentation. I leave it up to you guys to test your open source contribution skills and document it.\n",
    "\n",
    "### Running the following code will start scraping DPCC data and this code block takes a lot of time to execute. Give or take, it might go up till 2 or 3 days. Hit the stop button on toolbar if you run it and want to stop it, attempt to debug it by adding print statements in between etc... and figure out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in stNamesDict.items():\n",
    "\n",
    "    print(key)\n",
    "    save_dir = os.path.join(curr_dir,key)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.chdir(save_dir) #CHANGE dir\n",
    "#     print(os.getcwd())\n",
    "    uri = adv1Template+value\n",
    "    headers = {\n",
    "        'Origin': 'http://www.dpccairdata.com',\n",
    "        'Upgrade-Insecure-Requests':'1',\n",
    "        'Content-Type':'application/x-www-form-urlencoded',\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',\n",
    "        'X-DevTools-Emulate-Network-Conditions-Client-Id':'2B8F1DB78308223222DE718C809C036C',\n",
    "        'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'Referer': uri,\n",
    "        'Accept-Encoding':'gzip, deflate',\n",
    "        'Accept-Language':'en',\n",
    "        'Cookie':'PHPSESSID=8433578c40d503e4f046f054f60c531b'\n",
    "    }\n",
    "    \n",
    "    for param in adv1List:\n",
    "        \n",
    "        print(param)\n",
    "        l=[]\n",
    "        start = '2012-01-01 00:00' #CHANGE start date and time\n",
    "        endSec = datetime.strptime(start,'%Y-%m-%d %H:%M').timestamp()\n",
    "        while endSec<=time.time()-302400:\n",
    "            print(start)\n",
    "            endSec = datetime.strptime(start,'%Y-%m-%d %H:%M').timestamp() +604800-1 #end date and time set to 1 week ahead\n",
    "            end = datetime.fromtimestamp(endSec)\n",
    "            end = end.strftime('%Y-%m-%d %H:%M')\n",
    "            formFields = (\n",
    "                (r'parameters',param),\n",
    "                (r'fDate',start),\n",
    "                (r'eDate',end),\n",
    "                (r'duration',r'1'),\n",
    "                (r'graphType',r'Line'),\n",
    "                (r'submit',r'Search')\n",
    "            )\n",
    "\n",
    "            formDict = {}\n",
    "            for a,b in formFields:\n",
    "                formDict[a] = b\n",
    "\n",
    "            session = requests.Session()\n",
    "            resp = session.post(uri,headers=headers,data=formDict)\n",
    "            session.close()\n",
    "\n",
    "            spG = soup(resp.content,'lxml')\n",
    "            sps = spG.findAll('script')\n",
    "            spF = sps[-1]\n",
    "            spFF = str(spF)[str(spF).find('<graph caption'):str(spF).find('</graph>')+8]\n",
    "            asd = soup(spFF,'lxml').findAll('set')\n",
    "            for a in asd:\n",
    "                q = soup(str(a),'lxml').find('set')['name']\n",
    "                q = q.split(\" \")\n",
    "                w = soup(str(a),'lxml').find('set')['value']\n",
    "                l.append([q[0],q[1],w])\n",
    "            start = datetime.fromtimestamp(endSec+1) #start date and time set to 1 week ahead of previous\n",
    "            start = start.strftime('%Y-%m-%d %H:%M')\n",
    "#         print(l)\n",
    "        df = pd.DataFrame(l,columns = ['Date','Timestamp','value'])\n",
    "        df.to_csv(key+\"_\"+param+\".csv\") #store to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function I had used to clean the data after scraping it\n",
    "def cvrt(ts):\n",
    "    try:\n",
    "        return pd.to_datetime(ts)\n",
    "    except:\n",
    "        return pd.to_datetime(ts,format = '%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same: This is a function I had used to clean the data after scraping it\n",
    "def cleanDF2(df):\n",
    "    df['time'] = df['Date'] +\" \"+ df['Timestamp']\n",
    "    df['time'] = pd.to_datetime(df['time'],format = '%m/%d/%Y %H:%M:%S')\n",
    "    df = df.set_index('time')\n",
    "    df = df['value']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(n):\n",
    "    try:\n",
    "        float(n)   # Type-casting the string to `float`.\n",
    "                   # If string is not a valid `float`, \n",
    "                   # it'll raise `ValueError` exception\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDir(key):\n",
    "    return os.path.join(curr_dir,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code at your own risk. I haven't debugged it. Again, I leave it upto you guys to download the data,\n",
    "# figure out why the data needs to be cleaned and run this code block to see what it does\n",
    "for key in stNamesDict:\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for item in cols:\n",
    "        print(item)\n",
    "        dftemp = cleanDF(pd.read_csv(getDir(key,item)))\n",
    "        for i,t in dftemp.iteritems():\n",
    "            if is_number(t):\n",
    "                t = float(t)\n",
    "            else:\n",
    "                t = np.nan\n",
    "            i = cvrt(i)\n",
    "            df.loc[i,item] = t\n",
    "        df[item] = pd.to_numeric(df[item])\n",
    "        assert(df[item].dtype != 'object')\n",
    "    df = df.sort_index()\n",
    "    df.to_csv('Final/'+key+'.csv')\n",
    "    doneSt.append(key)\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I hope you guys enjoyed the workshops! Contact me if you want any other aspect you want covered in another class that you feel that I could live upto your expectations for and I will do my best! But for now, Good Luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
